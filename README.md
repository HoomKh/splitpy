# SplitPy

# ğŸ“š LangChain Text Splitters Toolkit

This module provides various document splitting strategies for preparing text to be processed by LLMs or NLP systems.

---

## âœ¨ Why split documents?

There are several practical reasons for splitting text into smaller, manageable chunks:

* âœ… Avoiding input length limits in LLMs (like OpenAI models)
* âœ… Improving semantic representation for long documents
* âœ… Enhancing search precision in RAG and embedding systems
* âœ… Making document processing more memory and compute efficient
* âœ… Enabling context windowing in chat systems and pipelines

---

## ğŸ§  Strategies included

| Splitter                                     | Type      | Token-aware | Recursive | Description                         |
| -------------------------------------------- | --------- | ----------- | --------- | ----------------------------------- |
| `character_text_splitter`                    | character | âŒ           | âŒ         | Basic fixed-size character chunks   |
| `recursive_character_text_splitter`          | character | âŒ           | âœ…         | Smarter structure-aware chunking    |
| `tiktoken_character_text_splitter`           | token     | âœ…           | âŒ         | Token-limited LLM-safe splitting    |
| `tiktoken_recursive_character_text_splitter` | token     | âœ…           | âœ…         | Best for OpenAI-compatible RAG      |
| `tiktoken_token_text_splitter`               | token     | âœ…           | âŒ         | Raw token chunking (fast but blind) |

---

## âš™ï¸ Example usage

```python
from tools.splitters import tiktoken_recursive_character_text_splitter

chunks = tiktoken_recursive_character_text_splitter(
    path="data/article.txt",
    chunk_size=200,
    chunk_overlap=30,
    encoding_name="cl100k_base"
)

print(chunks[0].page_content)
```

---

## ğŸ§ª When to use which?

| Use Case                     | Recommended Splitter                         |
| ---------------------------- | -------------------------------------------- |
| RAG + OpenAI                 | `tiktoken_recursive_character_text_splitter` |
| Basic NLP preprocessing      | `character_text_splitter`                    |
| Fast token slicing           | `tiktoken_token_text_splitter`               |
| Semantic chunking for search | `recursive_character_text_splitter`          |

---

## ğŸ“Œ Source reference

This implementation follows concepts from LangChainâ€™s official docs:
ğŸ”— [LangChain Docs: Text Splitters](https://python.langchain.com/docs/concepts/text_splitters/)
